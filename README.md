# QA-LLM Guard ğŸ›¡ï¸

Automated testing framework for Large Language Models (LLMs), using [DeepEval](https://github.com/confident-ai/deepeval).

## ğŸ“‘ Table of Contents

- [ğŸš€ Overview](#overview)
- [ğŸŒŸ Features](#features)
- [ğŸ› ï¸ Installation](#installation)
- [ğŸ—ï¸ Running tests](#running-tests)
- [ğŸ“š Directory Structure](#directory-structure)
- [ğŸ“œ License](#license)

## Overview

This project helps you:
- Define structured test cases for LLMs in Python
- Evaluate model responses using a local embedding-based metric
- Run tests via pytest with optional response caching
- Get detailed Allure reports for each test case

## Features

âœ… LLM test cases defined in Python  
âœ… Local embedding similarity metric (offline)  
âœ… OpenAI API support (configurable model)  
âœ… Fallback to Hugging Face models if no API key  
âœ… Response caching for faster repeat runs  
âœ… Parametrized pytest suite with Allure integration

## Installation

```bash
git clone https://github.com/your-username/qa-llm-guard.git
cd qa-llm-guard
python -m venv venv
source venv/bin/activate  # or .\venv\Scripts\activate on Windows
pip install -r requirements.txt
cp .env.example .env      # and fill in your OPENAI_API_KEY and OPENAI_MODEL if needed
```

## Running tests
Execute the full test suite and generate an Allure report:

```bash
pytest --update-cache   # fill or update response cache
pytest --cached         # run using cached responses
```
- --cached â€” use cached LLM responses
- --update-cache â€” update cache with new LLM responses

Allure results are written to allure-results/. To view the report locally, run:

```bash
allure serve allure-results
```

## Directory Structure

```bash
qa-llm-guard/
â”œâ”€â”€ llm_guard/              # Core logic
â”‚   â”œâ”€â”€ loader.py
â”‚   â”œâ”€â”€ cache.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ openai_model.py
â”‚   â””â”€â”€ metrics/
â”‚       â””â”€â”€ local_embedding.py
â”œâ”€â”€ test_cases/             # LLMTestCase definitions
â”‚   â”œâ”€â”€ test_geography.py
â”‚   â”œâ”€â”€ test_math.py
â”‚   â”œâ”€â”€ test_programming.py
â”‚   â”œâ”€â”€ test_history.py
â”‚   â””â”€â”€ test_creative.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py         # pytest options and cache fixture
â”‚   â””â”€â”€ test_llm_cases.py   # parametrized test for each case
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ pytest.ini
â””â”€â”€ README.md

```

## License
MIT License